// --------------------------------------
// Code generated by ChatGPT model o1 pro
// --------------------------------------

import { canvas, ctx, state } from "./state.js";

// Offscreen canvas + context
let offscreenCanvas = null;
let offscreenGpuContext = null;

// WEBGPU device/pipeline/buffer objects
let gpuDevice = null;
let gpuPipeline = null;
let gpuUniformBuffer = null;
let gpuBindGroup = null;

/**
 * Initialize WebGPU context/pipeline if supported
 */
export async function initWebGPU() {
    try {
        if (!('gpu' in navigator)) {
            console.warn('WebGPU not supported in this browser.');
            return false;
        }

        const adapter = await navigator.gpu.requestAdapter();
        if (!adapter) {
            console.warn('Failed to get GPU adapter.');
            return false;
        }

        gpuDevice = await adapter.requestDevice();
        if (!gpuDevice) {
            console.warn('Failed to request GPU device.');
            return false;
        }

        // ----------------------------------------------
        // 1. Create a hidden offscreen canvas + context
        // ----------------------------------------------
        offscreenCanvas = document.createElement('canvas');
        offscreenCanvas.style.display = 'none';
        document.body.appendChild(offscreenCanvas);

        offscreenGpuContext = offscreenCanvas.getContext('webgpu');
        if (!offscreenGpuContext) {
            console.warn('Could not get WebGPU context for offscreen canvas.');
            return false;
        }

        // Choose a preferred canvas format
        const format = navigator.gpu.getPreferredCanvasFormat();

        // Create our render pipeline
        gpuPipeline = gpuDevice.createRenderPipeline({
            layout: 'auto',
            vertex: {
                module: gpuDevice.createShaderModule({
                    code: wgslVertexShader
                }),
                entryPoint: 'main'
            },
            fragment: {
                module: gpuDevice.createShaderModule({
                    code: wgslFragmentShader
                }),
                entryPoint: 'main',
                targets: [{ format }]
            },
            primitive: {
                topology: 'triangle-strip',
                stripIndexFormat: undefined
            }
        });

        // Create a buffer for the uniform data.
        // We'll store centerX, centerY, scale, plus some padding, plus resolution as f32x2.
        gpuUniformBuffer = gpuDevice.createBuffer({
            size: 48, // enough space for 5 f32 + padding
            usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
        });

        gpuBindGroup = gpuDevice.createBindGroup({
            layout: gpuPipeline.getBindGroupLayout(0),
            entries: [
                {
                    binding: 0,
                    resource: {
                        buffer: gpuUniformBuffer
                    }
                }
            ]
        });

        return true;
    } catch (error) {
        console.warn(error);
        return false;
    }
}

/**
 * Render fractal with WebGPU into an offscreen canvas, then blit to the visible canvas.
 */
export function renderFractalWebGPU(scale = 1) {
    if (!gpuDevice || !gpuPipeline || !offscreenGpuContext) {
        console.error("WebGPU context not initialized properly");
        return;
    }

    // ------------------------------------
    // 2. Configure our offscreen canvas
    // ------------------------------------
    // We'll match the final size * scale. For example, if our main canvas is 800x600
    // and scale=0.5, then offscreen is 400x300, etc.
    const w = Math.floor(canvas.width * scale);
    const h = Math.floor(canvas.height * scale);

    offscreenCanvas.width = w;
    offscreenCanvas.height = h;

    const format = navigator.gpu.getPreferredCanvasFormat();
    offscreenGpuContext.configure({
        device: gpuDevice,
        format: format,
        alphaMode: 'premultiplied'
    });

    // ------------------------------------
    // 3. Write fractal parameters to GPU
    // ------------------------------------
    // We'll fill our uniform buffer with the current parameters.
    // Layout:
    //   offset 0: centerX (f32)
    //   offset 4: centerY (f32)
    //   offset 8: scale   (f32)
    //   offset 12: unused/padding (f32) [so next offset is multiple of 16]
    //   offset 16: resolution.x (f32)
    //   offset 20: resolution.y (f32)
    //
    const uniformArray = new Float32Array(6); // enough for centerX, centerY, scale, pad, resolution.x, resolution.y
    uniformArray[0] = state.x;     // centerX
    uniformArray[1] = state.y;     // centerY
    uniformArray[2] = state.scale; // scale
    uniformArray[3] = 0.0;         // pad (optional)
    uniformArray[4] = w;           // resolution.x
    uniformArray[5] = h;           // resolution.y

    gpuDevice.queue.writeBuffer(gpuUniformBuffer, 0, uniformArray);

    // Acquire a texture to render into (offscreen)
    const renderView = offscreenGpuContext.getCurrentTexture().createView();

    // Build the command pass
    const commandEncoder = gpuDevice.createCommandEncoder();
    const passEncoder = commandEncoder.beginRenderPass({
        colorAttachments: [
            {
                view: renderView,
                clearValue: { r: 0, g: 0, b: 0, a: 1 },
                loadOp: 'clear',
                storeOp: 'store',
            },
        ],
    });

    passEncoder.setPipeline(gpuPipeline);
    passEncoder.setBindGroup(0, gpuBindGroup);
    passEncoder.draw(4, 1, 0, 0); // 4 verts => full-screen quad
    passEncoder.end();

    const gpuCommands = commandEncoder.finish();
    gpuDevice.queue.submit([gpuCommands]);

    // ------------------------------------
    // 4. Blit from offscreen -> main canvas
    // ------------------------------------
    // Use the main canvas's 2D context to draw the offscreen image:
    // If you want a simple "centered" or "fit" approach, you can do:
    ctx.save();
    ctx.scale(1 / scale, 1 / scale);
    ctx.drawImage(offscreenCanvas, 0, 0);
    ctx.restore();
}

/* ---------------------------------------------------------
 * WGSL Shaders (unchanged)
 * --------------------------------------------------------- */

const wgslVertexShader = /* wgsl */ `
@vertex
fn main(@builtin(vertex_index) vertexIndex : u32) -> @builtin(position) vec4f {
    // We'll draw 2 triangles that cover the entire clip space:
    //   vertexIndex: 0,1,2,3 => positions in a strip
    let x = f32((vertexIndex & 1u) << 1u) - 1.0; // 0->-1, 1->1, 2->-1, 3->1
    let y = f32((vertexIndex & 2u)) - 1.0;      // 0->-1, 1->-1, 2->1, 3->1
    return vec4f(x, y, 0.0, 1.0);
}
`;

const wgslFragmentShader = /* wgsl */ `
struct FractalUniforms {
    centerX   : f32,
    centerY   : f32,
    scale     : f32,
    pad       : f32,          // keep alignment simple if you want a 16-byte boundary
    resolution: vec2<f32>
};

@group(0) @binding(0)
var<uniform> u : FractalUniforms;

@fragment
fn main(@builtin(position) fragCoord: vec4f) -> @location(0) vec4f {
    // pixel-based coordinate in f32
    let px = fragCoord.x;
    let py = fragCoord.y;

    let width  = u.resolution.x;
    let height = u.resolution.y;

    let scaleFactor = 4.0 / (width * u.scale);

    // Convert from pixel coords -> fractal coords
    let x0 = u.centerX + (px - 0.5 * width)  * scaleFactor;
    let y0 = u.centerY - (py - 0.5 * height) * scaleFactor;

    // Standard Mandelbrot iteration in f32
    var x : f32 = 0.0;
    var y : f32 = 0.0;

    let maxIter = 500;
    var escapeValue = maxIter;

    for (var i = 0; i < maxIter; i = i + 1) {
        let x2 = x*x - y*y + x0;
        let y2 = 2.0 * x*y + y0;
        x = x2;
        y = y2;

        if (x*x + y*y > 4.0) {
            escapeValue = i;
            break;
        }
    }

    if (escapeValue == maxIter) {
        // inside => black
        return vec4f(0.0, 0.0, 0.0, 1.0);
    } else {
        // outside => color = (c, c, 1)
        let c = 1.0 - f32(escapeValue) / f32(maxIter);
        return vec4f(c, c, 1.0, 1.0);
    }
}
`;
